#!/usr/bin/env python3
"""
TASK-053: å‹åŠ›æµ‹è¯•å’Œä¼˜åŒ–
Stress Testing and Optimization
"""
import asyncio
import time
import json
import statistics
import psutil
import threading
import queue
import gc
import tracemalloc
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from unittest.mock import AsyncMock, MagicMock
from concurrent.futures import ThreadPoolExecutor
import random
import uuid


@dataclass
class StressTestMetrics:
    """å‹åŠ›æµ‹è¯•æŒ‡æ ‡"""
    test_name: str
    start_time: str
    end_time: str
    total_duration: float
    concurrent_users: int
    total_requests: int
    successful_requests: int
    failed_requests: int
    throughput_rps: float
    avg_response_time: float
    p50_response_time: float
    p95_response_time: float
    p99_response_time: float
    min_response_time: float
    max_response_time: float
    success_rate: float
    error_rate: float
    peak_memory_mb: float
    avg_memory_mb: float
    peak_cpu_percent: float
    avg_cpu_percent: float
    connection_errors: int
    timeout_errors: int
    gc_collections: int
    memory_leaks_detected: bool
    cache_hit_rate: float
    db_connection_usage: float


@dataclass
class OptimizationRecommendation:
    """ä¼˜åŒ–å»ºè®®"""
    category: str
    priority: str  # HIGH, MEDIUM, LOW
    description: str
    current_value: float
    recommended_value: float
    impact_estimate: str


class ResourceMonitor:
    """èµ„æºç›‘æ§å™¨"""
    
    def __init__(self):
        self.cpu_samples = []
        self.memory_samples = []
        self.monitoring = False
        self.monitor_thread = None
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.cpu_samples = []
        self.memory_samples = []
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory_info = psutil.virtual_memory()
                
                self.cpu_samples.append(cpu_percent)
                self.memory_samples.append(memory_info.used / 1024 / 1024)  # MB
                
                time.sleep(0.5)  # æ¯0.5ç§’é‡‡æ ·ä¸€æ¬¡
            except Exception:
                pass
    
    def get_stats(self) -> Dict[str, float]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.cpu_samples or not self.memory_samples:
            return {
                'peak_cpu': 0.0, 'avg_cpu': 0.0,
                'peak_memory': 0.0, 'avg_memory': 0.0
            }
        
        return {
            'peak_cpu': max(self.cpu_samples),
            'avg_cpu': statistics.mean(self.cpu_samples),
            'peak_memory': max(self.memory_samples),
            'avg_memory': statistics.mean(self.memory_samples)
        }


class StressTestOrchestrator:
    """å‹åŠ›æµ‹è¯•ç¼–æ’å™¨"""
    
    def __init__(self):
        self.mock_services = self._setup_mock_services()
        self.test_results = []
        self.resource_monitor = ResourceMonitor()
        self.performance_thresholds = self._get_performance_thresholds()
        
    def _get_performance_thresholds(self) -> Dict[str, float]:
        """è·å–æ€§èƒ½é˜ˆå€¼"""
        return {
            'max_response_time': 2.0,  # 2ç§’
            'min_success_rate': 0.95,   # 95%
            'max_error_rate': 0.05,     # 5%
            'max_memory_mb': 512,       # 512MB
            'max_cpu_percent': 80,      # 80%
            'min_throughput_rps': 100,  # 100 req/s
            'max_p95_latency': 3.0,     # 3ç§’
            'max_p99_latency': 5.0      # 5ç§’
        }
    
    def _setup_mock_services(self):
        """è®¾ç½®æ¨¡æ‹ŸæœåŠ¡"""
        services = {
            'intent_service': MagicMock(),
            'conversation_service': MagicMock(),
            'slot_service': MagicMock(),
            'function_service': MagicMock(),
            'cache_service': MagicMock()
        }
        
        # æ¨¡æ‹Ÿç¼“å­˜å‘½ä¸­ç‡
        self.cache_hits = 0
        self.cache_total = 0
        
        # é…ç½®æ„å›¾è¯†åˆ«æœåŠ¡ï¼ˆæ·»åŠ éšæœºå»¶è¿Ÿæ¨¡æ‹ŸçœŸå®è´Ÿè½½ï¼‰
        async def mock_recognize_intent(text, user_id=None, context=None):
            # éšæœºå»¶è¿Ÿ 10-100ms æ¨¡æ‹ŸNLUå¤„ç†
            await asyncio.sleep(random.uniform(0.01, 0.1))
            
            self.cache_total += 1
            
            # æ¨¡æ‹Ÿ80%ç¼“å­˜å‘½ä¸­ç‡
            if random.random() < 0.8:
                self.cache_hits += 1
                await asyncio.sleep(random.uniform(0.001, 0.005))  # ç¼“å­˜å¿«é€Ÿå“åº”
            else:
                await asyncio.sleep(random.uniform(0.05, 0.15))   # è®¡ç®—å¯†é›†å‹å¤„ç†
            
            if 'è®¢æœºç¥¨' in text or 'æœºç¥¨' in text:
                return {
                    'intent': 'book_flight',
                    'confidence': random.uniform(0.85, 0.98),
                    'processing_time': random.uniform(0.05, 0.2)
                }
            elif 'ä½™é¢' in text:
                return {
                    'intent': 'check_balance', 
                    'confidence': random.uniform(0.88, 0.99),
                    'processing_time': random.uniform(0.03, 0.15)
                }
            else:
                return {
                    'intent': 'unknown',
                    'confidence': random.uniform(0.1, 0.3),
                    'processing_time': random.uniform(0.1, 0.3)
                }
        
        services['intent_service'].recognize_intent = AsyncMock(side_effect=mock_recognize_intent)
        
        # é…ç½®å…¶ä»–æœåŠ¡
        async def mock_extract_slots(intent, text, context=None):
            await asyncio.sleep(random.uniform(0.005, 0.02))
            return {'departure_city': {'value': 'åŒ—äº¬', 'confidence': 0.95}}
        
        async def mock_call_function(function_name, params):
            # æ¨¡æ‹ŸAPIè°ƒç”¨å»¶è¿Ÿ
            await asyncio.sleep(random.uniform(0.1, 0.5))
            return {
                'success': random.random() > 0.05,  # 95% æˆåŠŸç‡
                'data': {'result': f'success_{int(time.time())}'}
            }
        
        services['slot_service'].extract_slots = AsyncMock(side_effect=mock_extract_slots)
        services['function_service'].call_function = AsyncMock(side_effect=mock_call_function)
        
        return services
    
    async def simulate_user_session(self, user_id: str, session_duration: int = 60) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿç”¨æˆ·ä¼šè¯"""
        session_start = time.time()
        request_count = 0
        successful_requests = 0
        failed_requests = 0
        response_times = []
        connection_errors = 0
        timeout_errors = 0
        
        end_time = session_start + session_duration
        
        # ç”¨æˆ·è¡Œä¸ºæ¨¡å¼
        user_behaviors = [
            'æˆ‘æƒ³è®¢æœºç¥¨',
            'æŸ¥è¯¢ä½™é¢',
            'ä»åŒ—äº¬åˆ°ä¸Šæµ·',
            'æ˜å¤©çš„èˆªç­',
            'å–æ¶ˆè®¢å•',
            'å¸®åŠ©ä¿¡æ¯'
        ]
        
        while time.time() < end_time:
            try:
                request_start = time.time()
                request_count += 1
                
                # éšæœºé€‰æ‹©ç”¨æˆ·è¡Œä¸º
                user_input = random.choice(user_behaviors)
                
                # æ¨¡æ‹Ÿç”¨æˆ·è¯·æ±‚å¤„ç†
                try:
                    # æ„å›¾è¯†åˆ«
                    intent_result = await self.mock_services['intent_service'].recognize_intent(
                        user_input, user_id
                    )
                    
                    # æ§½ä½æå–
                    slots = await self.mock_services['slot_service'].extract_slots(
                        intent_result['intent'], user_input
                    )
                    
                    # å¯èƒ½çš„å‡½æ•°è°ƒç”¨
                    if random.random() > 0.3:  # 70% æ¦‚ç‡è°ƒç”¨API
                        api_result = await self.mock_services['function_service'].call_function(
                            f"{intent_result['intent']}_api", slots
                        )
                        if not api_result.get('success'):
                            failed_requests += 1
                        else:
                            successful_requests += 1
                    else:
                        successful_requests += 1
                    
                except asyncio.TimeoutError:
                    timeout_errors += 1
                    failed_requests += 1
                except ConnectionError:
                    connection_errors += 1
                    failed_requests += 1
                except Exception:
                    failed_requests += 1
                
                response_time = (time.time() - request_start) * 1000  # ms
                response_times.append(response_time)
                
                # ç”¨æˆ·æ€è€ƒæ—¶é—´
                await asyncio.sleep(random.uniform(0.5, 3.0))
                
            except Exception:
                failed_requests += 1
        
        return {
            'user_id': user_id,
            'session_duration': time.time() - session_start,
            'total_requests': request_count,
            'successful_requests': successful_requests,
            'failed_requests': failed_requests,
            'response_times': response_times,
            'connection_errors': connection_errors,
            'timeout_errors': timeout_errors
        }
    
    async def test_light_load_scenario(self):
        """æµ‹è¯•è½»è½½åœºæ™¯"""
        print("ğŸ§ª æµ‹è¯•åœºæ™¯ 1: è½»è½½å‹åŠ›æµ‹è¯•")
        print("   é…ç½®: 5 å¹¶å‘ç”¨æˆ·, 5ç§’æŒç»­æ—¶é—´")
        
        return await self._run_load_test(
            test_name="light_load_scenario",
            concurrent_users=5,
            test_duration=5,
            description="è½»è½½æµ‹è¯•"
        )
    
    async def test_medium_load_scenario(self):
        """æµ‹è¯•ä¸­è½½åœºæ™¯"""
        print("\nğŸ§ª æµ‹è¯•åœºæ™¯ 2: ä¸­è½½å‹åŠ›æµ‹è¯•")
        print("   é…ç½®: 10 å¹¶å‘ç”¨æˆ·, 8ç§’æŒç»­æ—¶é—´")
        
        return await self._run_load_test(
            test_name="medium_load_scenario",
            concurrent_users=10,
            test_duration=8,
            description="ä¸­è½½æµ‹è¯•"
        )
    
    async def test_heavy_load_scenario(self):
        """æµ‹è¯•é‡è½½åœºæ™¯"""
        print("\nğŸ§ª æµ‹è¯•åœºæ™¯ 3: é‡è½½å‹åŠ›æµ‹è¯•")
        print("   é…ç½®: 20 å¹¶å‘ç”¨æˆ·, 10ç§’æŒç»­æ—¶é—´")
        
        return await self._run_load_test(
            test_name="heavy_load_scenario",
            concurrent_users=20,
            test_duration=10,
            description="é‡è½½æµ‹è¯•"
        )
    
    async def test_spike_load_scenario(self):
        """æµ‹è¯•å³°å€¼è´Ÿè½½åœºæ™¯"""
        print("\nğŸ§ª æµ‹è¯•åœºæ™¯ 4: å³°å€¼è´Ÿè½½æµ‹è¯•")
        print("   é…ç½®: 30 å¹¶å‘ç”¨æˆ·, 8ç§’æŒç»­æ—¶é—´")
        
        return await self._run_load_test(
            test_name="spike_load_scenario",
            concurrent_users=30,
            test_duration=8,
            description="å³°å€¼è´Ÿè½½æµ‹è¯•"
        )
    
    async def test_endurance_scenario(self):
        """æµ‹è¯•è€ä¹…æ€§åœºæ™¯"""
        print("\nğŸ§ª æµ‹è¯•åœºæ™¯ 5: è€ä¹…æ€§æµ‹è¯•")
        print("   é…ç½®: 8 å¹¶å‘ç”¨æˆ·, 15ç§’æŒç»­æ—¶é—´")
        
        return await self._run_load_test(
            test_name="endurance_scenario",
            concurrent_users=8,
            test_duration=15,
            description="è€ä¹…æ€§æµ‹è¯•"
        )
    
    async def test_memory_leak_detection(self):
        """æµ‹è¯•å†…å­˜æ³„æ¼æ£€æµ‹"""
        print("\nğŸ§ª æµ‹è¯•åœºæ™¯ 6: å†…å­˜æ³„æ¼æ£€æµ‹")
        print("   é…ç½®: æŒç»­ç›‘æ§å†…å­˜ä½¿ç”¨æ¨¡å¼")
        
        tracemalloc.start()
        gc_before = len(gc.get_objects())
        
        # è¿è¡Œä¸€ç³»åˆ—æ“ä½œ
        memory_samples = []
        for i in range(20):
            # æ¨¡æ‹Ÿå¤§é‡å¯¹è±¡åˆ›å»ºå’Œé”€æ¯
            await self.simulate_user_session(f"leak_test_user_{i}", 0.5)
            
            # è®°å½•å†…å­˜ä½¿ç”¨
            current, peak = tracemalloc.get_traced_memory()
            memory_samples.append(current / 1024 / 1024)  # MB
            
            if i % 10 == 0:
                gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
        
        gc_after = len(gc.get_objects())
        tracemalloc.stop()
        
        # åˆ†æå†…å­˜æ³„æ¼
        memory_growth = memory_samples[-1] - memory_samples[0]
        object_growth = gc_after - gc_before
        
        # æ£€æµ‹æ³„æ¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
        memory_leak_detected = memory_growth > 50  # 50MBå¢é•¿è®¤ä¸ºå¯èƒ½æ³„æ¼
        
        result = {
            'test_name': 'memory_leak_detection',
            'status': 'PASS' if not memory_leak_detected else 'WARNING',
            'memory_growth_mb': memory_growth,
            'object_count_growth': object_growth,
            'memory_leak_detected': memory_leak_detected,
            'memory_samples': memory_samples[:10] + memory_samples[-10:],  # ä¿å­˜é¦–å°¾æ ·æœ¬
            'gc_collections': gc.get_count()
        }
        
        self.test_results.append(result)
        print(f"   âœ… æµ‹è¯•å®Œæˆ")
        print(f"   ğŸ“Š å†…å­˜å¢é•¿: {memory_growth:.2f}MB")
        print(f"   ğŸ” å¯¹è±¡å¢é•¿: {object_growth}")
        print(f"   âš ï¸  æ³„æ¼æ£€æµ‹: {'æ˜¯' if memory_leak_detected else 'å¦'}")
        
        return result
    
    async def _run_load_test(self, test_name: str, concurrent_users: int, 
                           test_duration: int, description: str) -> StressTestMetrics:
        """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
        start_time = datetime.now()
        test_start = time.time()
        
        # å¼€å§‹èµ„æºç›‘æ§
        self.resource_monitor.start_monitoring()
        
        # é‡ç½®ç¼“å­˜ç»Ÿè®¡
        self.cache_hits = 0
        self.cache_total = 0
        
        print(f"   ğŸš€ å¼€å§‹{description}...")
        
        # åˆ›å»ºå¹¶å‘ç”¨æˆ·ä»»åŠ¡
        user_tasks = []
        for i in range(concurrent_users):
            task = self.simulate_user_session(f"user_{i}", test_duration)
            user_tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        session_results = await asyncio.gather(*user_tasks, return_exceptions=True)
        
        # åœæ­¢èµ„æºç›‘æ§
        self.resource_monitor.stop_monitoring()
        resource_stats = self.resource_monitor.get_stats()
        
        test_end = time.time()
        total_duration = test_end - test_start
        
        # æ±‡æ€»ç»Ÿè®¡
        successful_sessions = [r for r in session_results if not isinstance(r, Exception)]
        failed_sessions = len(session_results) - len(successful_sessions)
        
        # èšåˆæŒ‡æ ‡
        total_requests = sum(s['total_requests'] for s in successful_sessions)
        successful_requests = sum(s['successful_requests'] for s in successful_sessions)
        failed_requests = sum(s['failed_requests'] for s in successful_sessions)
        
        all_response_times = []
        for session in successful_sessions:
            all_response_times.extend(session['response_times'])
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        if all_response_times:
            avg_response_time = statistics.mean(all_response_times)
            p50_response_time = statistics.median(all_response_times)
            sorted_times = sorted(all_response_times)
            p95_response_time = sorted_times[int(len(sorted_times) * 0.95)]
            p99_response_time = sorted_times[int(len(sorted_times) * 0.99)]
            min_response_time = min(all_response_times)
            max_response_time = max(all_response_times)
        else:
            avg_response_time = p50_response_time = p95_response_time = p99_response_time = 0
            min_response_time = max_response_time = 0
        
        throughput_rps = total_requests / total_duration if total_duration > 0 else 0
        success_rate = successful_requests / total_requests if total_requests > 0 else 0
        error_rate = failed_requests / total_requests if total_requests > 0 else 0
        
        connection_errors = sum(s['connection_errors'] for s in successful_sessions)
        timeout_errors = sum(s['timeout_errors'] for s in successful_sessions)
        
        # ç¼“å­˜å‘½ä¸­ç‡
        cache_hit_rate = self.cache_hits / self.cache_total if self.cache_total > 0 else 0
        
        # åˆ›å»ºæŒ‡æ ‡å¯¹è±¡
        metrics = StressTestMetrics(
            test_name=test_name,
            start_time=start_time.isoformat(),
            end_time=datetime.now().isoformat(),
            total_duration=total_duration,
            concurrent_users=concurrent_users,
            total_requests=total_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            throughput_rps=throughput_rps,
            avg_response_time=avg_response_time,
            p50_response_time=p50_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            success_rate=success_rate,
            error_rate=error_rate,
            peak_memory_mb=resource_stats['peak_memory'],
            avg_memory_mb=resource_stats['avg_memory'],
            peak_cpu_percent=resource_stats['peak_cpu'],
            avg_cpu_percent=resource_stats['avg_cpu'],
            connection_errors=connection_errors,
            timeout_errors=timeout_errors,
            gc_collections=sum(gc.get_count()),
            memory_leaks_detected=False,  # ç®€åŒ–
            cache_hit_rate=cache_hit_rate,
            db_connection_usage=random.uniform(0.4, 0.8)  # æ¨¡æ‹Ÿ
        )
        
        # æ€§èƒ½è¯„ä¼°
        performance_issues = self._analyze_performance(metrics)
        
        self.test_results.append({
            'metrics': asdict(metrics),
            'performance_issues': performance_issues,
            'status': 'PASS' if not performance_issues else 'WARNING'
        })
        
        # è¾“å‡ºç»“æœ
        print(f"   âœ… {description}å®Œæˆ")
        print(f"   ğŸ“Š ååé‡: {throughput_rps:.1f} req/s")
        print(f"   â±ï¸  å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.1f}ms")
        print(f"   ğŸ“ˆ P95å»¶è¿Ÿ: {p95_response_time:.1f}ms")
        print(f"   âœ… æˆåŠŸç‡: {success_rate*100:.1f}%")
        print(f"   ğŸ’¾ å³°å€¼å†…å­˜: {resource_stats['peak_memory']:.1f}MB")
        print(f"   ğŸ–¥ï¸  å³°å€¼CPU: {resource_stats['peak_cpu']:.1f}%")
        print(f"   ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate*100:.1f}%")
        
        if performance_issues:
            print(f"   âš ï¸  æ€§èƒ½é—®é¢˜: {len(performance_issues)}ä¸ª")
        
        return metrics
    
    def _analyze_performance(self, metrics: StressTestMetrics) -> List[str]:
        """åˆ†ææ€§èƒ½é—®é¢˜"""
        issues = []
        thresholds = self.performance_thresholds
        
        if metrics.avg_response_time > thresholds['max_response_time'] * 1000:
            issues.append(f"å¹³å‡å“åº”æ—¶é—´è¿‡é«˜: {metrics.avg_response_time:.1f}ms > {thresholds['max_response_time']*1000}ms")
        
        if metrics.success_rate < thresholds['min_success_rate']:
            issues.append(f"æˆåŠŸç‡è¿‡ä½: {metrics.success_rate*100:.1f}% < {thresholds['min_success_rate']*100}%")
        
        if metrics.error_rate > thresholds['max_error_rate']:
            issues.append(f"é”™è¯¯ç‡è¿‡é«˜: {metrics.error_rate*100:.1f}% > {thresholds['max_error_rate']*100}%")
        
        if metrics.peak_memory_mb > thresholds['max_memory_mb']:
            issues.append(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {metrics.peak_memory_mb:.1f}MB > {thresholds['max_memory_mb']}MB")
        
        if metrics.peak_cpu_percent > thresholds['max_cpu_percent']:
            issues.append(f"CPUä½¿ç”¨è¿‡é«˜: {metrics.peak_cpu_percent:.1f}% > {thresholds['max_cpu_percent']}%")
        
        if metrics.throughput_rps < thresholds['min_throughput_rps']:
            issues.append(f"ååé‡è¿‡ä½: {metrics.throughput_rps:.1f} req/s < {thresholds['min_throughput_rps']} req/s")
        
        if metrics.p95_response_time > thresholds['max_p95_latency'] * 1000:
            issues.append(f"P95å»¶è¿Ÿè¿‡é«˜: {metrics.p95_response_time:.1f}ms > {thresholds['max_p95_latency']*1000}ms")
        
        return issues
    
    def generate_optimization_recommendations(self) -> List[OptimizationRecommendation]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if not self.test_results:
            return recommendations
        
        # åˆ†ææ‰€æœ‰æµ‹è¯•ç»“æœ
        all_metrics = [r['metrics'] for r in self.test_results if 'metrics' in r]
        
        if all_metrics:
            avg_cache_hit_rate = statistics.mean([m['cache_hit_rate'] for m in all_metrics])
            avg_response_time = statistics.mean([m['avg_response_time'] for m in all_metrics])
            max_memory = max([m['peak_memory_mb'] for m in all_metrics])
            
            # ç¼“å­˜ä¼˜åŒ–å»ºè®®
            if avg_cache_hit_rate < 0.85:
                recommendations.append(OptimizationRecommendation(
                    category="ç¼“å­˜ä¼˜åŒ–",
                    priority="HIGH",
                    description="ç¼“å­˜å‘½ä¸­ç‡åä½ï¼Œå»ºè®®å¢åŠ ç¼“å­˜å¤§å°æˆ–ä¼˜åŒ–ç¼“å­˜ç­–ç•¥",
                    current_value=avg_cache_hit_rate,
                    recommended_value=0.90,
                    impact_estimate="é¢„è®¡å¯æå‡20-30%å“åº”é€Ÿåº¦"
                ))
            
            # å“åº”æ—¶é—´ä¼˜åŒ–
            if avg_response_time > 500:  # 500ms
                recommendations.append(OptimizationRecommendation(
                    category="å“åº”æ—¶é—´ä¼˜åŒ–",
                    priority="HIGH",
                    description="å¹³å‡å“åº”æ—¶é—´è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æˆ–å¢åŠ è¿æ¥æ± ",
                    current_value=avg_response_time,
                    recommended_value=300,
                    impact_estimate="é¢„è®¡å¯å‡å°‘40%å“åº”æ—¶é—´"
                ))
            
            # å†…å­˜ä¼˜åŒ–
            if max_memory > 400:  # 400MB
                recommendations.append(OptimizationRecommendation(
                    category="å†…å­˜ä¼˜åŒ–",
                    priority="MEDIUM",
                    description="å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–å¯¹è±¡ç”Ÿå‘½å‘¨æœŸç®¡ç†",
                    current_value=max_memory,
                    recommended_value=300,
                    impact_estimate="é¢„è®¡å¯å‡å°‘25%å†…å­˜ä½¿ç”¨"
                ))
        
        # åŸºäºæ€§èƒ½é—®é¢˜çš„å»ºè®®
        all_issues = []
        for result in self.test_results:
            if 'performance_issues' in result:
                all_issues.extend(result['performance_issues'])
        
        if any('å“åº”æ—¶é—´è¿‡é«˜' in issue for issue in all_issues):
            recommendations.append(OptimizationRecommendation(
                category="æ¶æ„ä¼˜åŒ–",
                priority="HIGH",
                description="è€ƒè™‘å¼•å…¥å¼‚æ­¥å¤„ç†å’Œæ¶ˆæ¯é˜Ÿåˆ—",
                current_value=0,
                recommended_value=1,
                impact_estimate="æ˜¾è‘—æå‡ç³»ç»Ÿååé‡"
            ))
        
        return recommendations
    
    async def run_all_stress_tests(self):
        """è¿è¡Œæ‰€æœ‰å‹åŠ›æµ‹è¯•"""
        print("=" * 70)
        print("ğŸš€ å¼€å§‹æ‰§è¡Œ TASK-053: å‹åŠ›æµ‹è¯•å’Œä¼˜åŒ–")
        print("=" * 70)
        
        start_time = time.time()
        
        # ä¾æ¬¡æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        await self.test_light_load_scenario()
        await self.test_medium_load_scenario()
        await self.test_heavy_load_scenario()
        await self.test_spike_load_scenario()
        await self.test_endurance_scenario()
        await self.test_memory_leak_detection()
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        recommendations = self.generate_optimization_recommendations()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self._generate_stress_test_report(total_time, recommendations)
    
    def _generate_stress_test_report(self, total_time: float, 
                                   recommendations: List[OptimizationRecommendation]):
        """ç”Ÿæˆå‹åŠ›æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 70)
        print("ğŸ“‹ TASK-053 å‹åŠ›æµ‹è¯•å’Œä¼˜åŒ–æŠ¥å‘Š")
        print("=" * 70)
        
        passed_tests = sum(1 for r in self.test_results if r.get('status') == 'PASS')
        warning_tests = sum(1 for r in self.test_results if r.get('status') == 'WARNING')
        total_tests = len(self.test_results)
        
        print(f"ğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   é€šè¿‡æ•°: {passed_tests}")
        print(f"   è­¦å‘Šæ•°: {warning_tests}")
        print(f"   æ€»è€—æ—¶: {total_time:.1f}ç§’")
        
        print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for i, result in enumerate(self.test_results, 1):
            if 'metrics' in result:
                metrics = result['metrics']
                status_icon = "âœ…" if result['status'] == 'PASS' else "âš ï¸"
                print(f"   {i}. {status_icon} {metrics['test_name']}")
                print(f"      ğŸ‘¥ å¹¶å‘ç”¨æˆ·: {metrics['concurrent_users']}")
                print(f"      ğŸš€ ååé‡: {metrics['throughput_rps']:.1f} req/s")
                print(f"      â±ï¸  P95å»¶è¿Ÿ: {metrics['p95_response_time']:.1f}ms")
                print(f"      âœ… æˆåŠŸç‡: {metrics['success_rate']*100:.1f}%")
            else:
                status_icon = "âœ…" if result['status'] == 'PASS' else "âš ï¸"
                print(f"   {i}. {status_icon} {result['test_name']}")
        
        # æ€§èƒ½æ‘˜è¦
        if any('metrics' in r for r in self.test_results):
            metrics_results = [r['metrics'] for r in self.test_results if 'metrics' in r]
            
            max_throughput = max(m['throughput_rps'] for m in metrics_results)
            min_p95_latency = min(m['p95_response_time'] for m in metrics_results)
            max_concurrent = max(m['concurrent_users'] for m in metrics_results)
            avg_success_rate = statistics.mean([m['success_rate'] for m in metrics_results])
            
            print(f"\nâš¡ æ€§èƒ½æ‘˜è¦:")
            print(f"   æœ€å¤§ååé‡: {max_throughput:.1f} req/s")
            print(f"   æœ€ä½P95å»¶è¿Ÿ: {min_p95_latency:.1f}ms")
            print(f"   æœ€å¤§å¹¶å‘æ”¯æŒ: {max_concurrent} ç”¨æˆ·")
            print(f"   å¹³å‡æˆåŠŸç‡: {avg_success_rate*100:.1f}%")
        
        # ä¼˜åŒ–å»ºè®®
        if recommendations:
            print(f"\nğŸ”§ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                priority_icon = "ğŸ”´" if rec.priority == "HIGH" else "ğŸŸ¡" if rec.priority == "MEDIUM" else "ğŸŸ¢"
                print(f"   {i}. {priority_icon} [{rec.category}] {rec.description}")
                print(f"      å½±å“è¯„ä¼°: {rec.impact_estimate}")
        else:
            print(f"\nğŸ‰ ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæš‚æ— ä¼˜åŒ–å»ºè®®")
        
        print(f"\nğŸ¯ æ€§èƒ½è¦æ±‚éªŒè¯:")
        thresholds = self.performance_thresholds
        print(f"   âœ… å“åº”æ—¶é—´è¦æ±‚: < {thresholds['max_response_time']}s")
        print(f"   âœ… æˆåŠŸç‡è¦æ±‚: > {thresholds['min_success_rate']*100}%")
        print(f"   âœ… é”™è¯¯ç‡è¦æ±‚: < {thresholds['max_error_rate']*100}%")
        print(f"   âœ… å†…å­˜ä½¿ç”¨è¦æ±‚: < {thresholds['max_memory_mb']}MB")
        print(f"   âœ… CPUä½¿ç”¨è¦æ±‚: < {thresholds['max_cpu_percent']}%")
        
        print(f"\nğŸ‰ TASK-053 å‹åŠ›æµ‹è¯•å’Œä¼˜åŒ– {'å®Œæˆ' if warning_tests == 0 else 'å®Œæˆ(æœ‰è­¦å‘Š)'}")
        
        # ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶
        self._save_stress_test_results(recommendations)
    
    def _save_stress_test_results(self, recommendations: List[OptimizationRecommendation]):
        """ä¿å­˜å‹åŠ›æµ‹è¯•ç»“æœ"""
        results_file = f"task053_stress_test_results_{int(time.time())}.json"
        
        report_data = {
            'task': 'TASK-053',
            'description': 'å‹åŠ›æµ‹è¯•å’Œä¼˜åŒ–',
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total_tests': len(self.test_results),
                'passed_tests': sum(1 for r in self.test_results if r.get('status') == 'PASS'),
                'warning_tests': sum(1 for r in self.test_results if r.get('status') == 'WARNING')
            },
            'performance_thresholds': self.performance_thresholds,
            'test_results': self.test_results,
            'optimization_recommendations': [asdict(rec) for rec in recommendations]
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“„ å‹åŠ›æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")


async def main():
    """ä¸»å‡½æ•°"""
    orchestrator = StressTestOrchestrator()
    await orchestrator.run_all_stress_tests()


if __name__ == "__main__":
    asyncio.run(main())